Formula 1 Pit Stop & Driver Ranking Predictions
Author: Aya Tarist
Course: ANOP 330 (Prof. Bailey)
Date: August 2023

üìñ Project Overview
This repository contains all materials for a two‚Äêpart Formula 1 Machine Learning capstone:

Pit Stop Classification (Belgian Grand Prix 2022)

Use FastF1 telemetry and timing data to engineer a binary target (PitStopOccurred) for each lap.

Conduct Exploratory Data Analysis (EDA) to identify key predictors (LapTime, TyreLife, Speed metrics, Compound, etc.).

Prepare features and visualize distributions, correlations, and class imbalance.

(Next steps: build and evaluate a supervised classification model.)

Driver Ranking Prediction (2009‚Äì2022 Seasons)

Use Ergast API data (drivers, constructors, race results, points) to predict each driver‚Äôs finishing rank in a race.

Preprocess dates, fill missing values, and engineer features (driver age, DNF flags, concatenated driver names, etc.).

Perform EDA to uncover trends in team and driver performance, correlation structure, and feature importance.

Train and compare three models (Logistic Regression, Decision Tree, Random Forest) using Stratified K-Fold CV.

Evaluate accuracy, F1-score, precision, recall, and balanced accuracy before and after hyperparameter tuning.

üóÇ File Structure
rust
Copy
Edit
‚îú‚îÄ‚îÄ README.md                                  ‚Üê This documentation
‚îú‚îÄ‚îÄ F1_Rankings.ipynb                          ‚Üê Jupyter notebook with full workflow for driver ranking prediction
‚îú‚îÄ‚îÄ Project Report - EDA.pdf                   ‚Üê PDF: Exploratory Data Analysis for Pit Stop Classification
‚îú‚îÄ‚îÄ Project Report - Results & Analysis .pdf   ‚Üê PDF: Data Prep, EDA, Modeling & Results for Driver Rankings
‚îú‚îÄ‚îÄ data/                                      ‚Üê (Optional) Raw CSV exports or fetched datasets
‚îÇ   ‚îú‚îÄ‚îÄ belgian_gp_2022.csv                    ‚Üê Telemetry/timing data for Belgian GP (2022)
‚îÇ   ‚îî‚îÄ‚îÄ f1_2009_2022.csv                       ‚Üê Aggregated Ergast API data for seasons 2009‚Äì2022
‚îî‚îÄ‚îÄ requirements.txt                           ‚Üê Python dependencies and versions
Note: If your local copy does not include data/, fetch data via the FastF1 package (for 2022 Belgian GP) and the Ergast API (for multi‚Äêseason data).

üöÄ How to Run
1. Create a Conda (or virtualenv) Environment
bash
Copy
Edit
conda create -n f1_ml python=3.9
conda activate f1_ml
pip install -r requirements.txt
Contents of requirements.txt:

shell
Copy
Edit
fastf1>=2.4
pandas>=1.3
numpy>=1.21
matplotlib>=3.4
seaborn>=0.11
scikit-learn>=1.0
jupyterlab>=3.0
(If you prefer, install these packages manually with pip install fastf1 pandas numpy matplotlib seaborn scikit-learn jupyterlab.)

2. Pit Stop EDA Notebook
Launch JupyterLab (or Jupyter Notebook):

bash
Copy
Edit
jupyter lab
Open Project Report - EDA.pdf to read the write-up and view plots.

(Optional) If you have the Python‚Äêbased EDA notebook (not included in PDF), open it to explore code cells and replicate all visualizations:

bash
Copy
Edit
jupyter lab F1_PitStop_EDA.ipynb
Key Outputs:

Class distribution of PitStopOccurred (90% no stop, 10% stop).

Histograms and box plots for LapTime, TyreLife, Compound, Stint.

Correlation heatmap showing relationships between speed metrics, lap times, tyre life, and pit stops.

3. Driver Ranking Prediction Notebook
Open F1_Rankings.ipynb in JupyterLab:

bash
Copy
Edit
jupyter lab F1_Rankings.ipynb
Walk Through Sections:

Data Ingestion & Cleaning (2009‚Äì2022 Ergast data):

Converts driverDateOfBirth & raceDate to datetime.

Converts numeric columns (e.g., driverFastestLapSpeed) to numeric dtype.

Concatenates driverForename + driverSurname ‚Üí driverName.

Calculates driverAge (today‚Äôs date minus birthdate).

Creates driverDnf/constructorDnf flags based on result status codes.

Fills missing numeric columns (points, grid positions) with zeros.

Exploratory Data Analysis:

Team points‚Äêper‚Äêrace trends (e.g., Mercedes vs. Ferrari vs. Red Bull).

Driver vs. constructor championship point time series.

Correlation matrix (highlighting driverStartGridPos ‚Üî driverFinalGridPos, etc.).

Feature Engineering:

Select features with high correlation to driverFinalRank:
season, driverId, constructorId, driverStartGridPos, driverFinalGridPos, driverChampionshipStandingPosition, constructorChampionshipStandingPosition.

Modeling & Evaluation:

Splits data (80% train / 20% test).

Uses 10-fold Stratified K-Fold CV on training.

Trains three classifiers:

Logistic Regression

Decision Tree

Random Forest

Reports accuracy, F1-score, precision, recall, balanced accuracy before and after hyperparameter tuning (GridSearchCV).

Summary of best hyperparameters and model performance (e.g., Decision Tree ‚âà 85% accuracy, Random Forest ‚âà 85% accuracy and F1).

Key Outputs:

Table 1: Best hyperparameters after tuning (Logistic Regression: C=10, penalty=l1; Decision Tree: default; Random Forest: max_depth=30, n_estimators=200).

Table 2 / Table 3: Model metrics (accuracy, F1, precision, recall, balanced accuracy) pre- and post-tuning.

Scatter plots showing LapTime vs. TyreLife, boxplots of lap times, bar charts of compound usage, correlation heatmap, and time-series plots for team/driver points by season.

üìä Results & Key Insights
Pit Stop EDA (Project Report - EDA.pdf)
Class Imbalance: 714 laps without pit stops (90.15%), 78 laps with pit stops (9.85%).

Lap Time Distribution: Median ‚âà 125 seconds; outliers likely due to safety cars or incidents.

TyreLife Distribution: Right-skewed‚Äîmost tyres changed early, few long stints.

Compound Usage: ‚ÄúMEDIUM‚Äù used most often, followed by ‚ÄúHARD‚Äù and ‚ÄúSOFT.‚Äù

Correlation:

LapTime has moderate positive correlation (0.51) with PitStopOccurred.

SpeedI2 has strong negative correlation‚Äîhigher speed laps rarely coincide with pit stops.

TyreLife correlation is low, unexpectedly.

Next Steps: Incorporate external weather data (e.g., manual merge of track & race conditions), refine feature engineering, and train classification models.

Driver Ranking Predictions (Project Report - Results & Analysis .pdf)
Data Cleaning:

Converted date strings to datetime.

Filled missing numeric values with zero (zero points or no grid position).

Engineered driverDnf and constructorDnf flags.

EDA Findings:

Team Trends: Mercedes dominates points per race; Ferrari & Red Bull in a close rivalry; midfield teams (McLaren, Renault, AlphaTauri/Williams) cluster together; smaller teams (Haas, Toyota, BMW Sauber, Honda) lag behind.

Driver vs. Constructor Points: Drivers‚Äô success depends on team performance; older drivers tend to score fewer points.

Correlation Matrix:

driverStartGridPos ‚Üî driverFinalGridPos (r ‚âà 0.59).

driverFinalGridPos ‚Üî driverRacePoints (r ‚âà 0.44).

driverAge ‚Üî driverChampionshipStandingPoints (r ‚âà ‚àí0.61).

Modeling Results:

Before Tuning:

Logistic Regression ‚Äì Accuracy: 20.5%, F1: 0.168.

Decision Tree ‚Äì Accuracy: 85.06%, F1: 0.772.

Random Forest ‚Äì Accuracy: 81.77%, F1: 0.741.

After Tuning:

Logistic Regression ‚Äì Accuracy: 21.16%, F1: 0.190.

Decision Tree ‚Äì Accuracy: 82.27%, F1: 0.823.

Random Forest ‚Äì Accuracy: 85.42%, F1: 0.851.

Best Model: Random Forest (post-tuning) achieved highest balanced performance (Accuracy: 85.4%, F1: 0.851, Balanced Accuracy: 0.777).

Interpretation:

Decision trees (single vs. ensemble) capture non‚Äêlinear interactions (track conditions, driver skill, team strategy) better than Logistic Regression.

Random Forest‚Äôs ensemble averaging reduces overfitting and yields the strongest generalization.

‚öôÔ∏è Dependencies
Below is a template requirements.txt for easy environment setup. Adjust versions as needed.

shell
Copy
Edit
fastf1>=2.4
pandas>=1.3
numpy>=1.21
matplotlib>=3.4
seaborn>=0.11
scikit-learn>=1.0
jupyterlab>=3.0
To install:

bash
Copy
Edit
pip install -r requirements.txt
ü§ñ Extending the Work
Pit Stop Model:

Add weather features (temperature, precipitation, track conditions) by merging external APIs or CSVs.

Use class‚Äêimbalance techniques (SMOTE, class weights) to improve classification.

Compare different classifiers (Random Forest, XGBoost, Logistic Regression with L1/L2 regularization).

Ranking Model:

Introduce advanced features (constructor budget, driver experience metrics, track length, lap count).

Explore multi‚Äêclass ranking as ordinal regression.

Deploy a model in a lightweight web app for end‚Äêusers to query predicted finishing positions given driver, team, and qualifying data.

